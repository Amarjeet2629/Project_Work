{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amarjeet-pc/Desktop'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mCP\u001b[0m/  \u001b[01;34mml-coursera-python-assignments\u001b[0m/  \u001b[01;34mtext_gcn\u001b[0m/  Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amarjeet-pc/Desktop/text_gcn\n"
     ]
    }
   ],
   "source": [
    "cd text_gcn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow.py           inits.py      plot_window.py   utils.py\r\n",
      "build_corpus.py  layers.py     prepare_data.py  visualize.py\r\n",
      "build_graph.py   metrics.py    README.md        visualize_words.py\r\n",
      "\u001b[0m\u001b[01;34mdata\u001b[0m/            models.py     remove_words.py  wordnet.py\r\n",
      "doc2vec.py       plot_dim.py   \u001b[01;34mresults\u001b[0m/\r\n",
      "__init__.py      plot_prop.py  train.py\r\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-d37b937bb807>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-d37b937bb807>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python remove_words.py 20ng\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python remove_words.py 20ng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from nltk.corpus import stopwords\r\n",
      "import nltk\r\n",
      "from nltk.wsd import lesk\r\n",
      "from nltk.corpus import wordnet as wn\r\n",
      "from utils import clean_str, loadWord2Vec\r\n",
      "import sys\r\n",
      "\r\n",
      "if len(sys.argv) != 2:\r\n",
      "\tsys.exit(\"Use: python remove_words.py <dataset>\")\r\n",
      "\r\n",
      "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\r\n",
      "dataset = sys.argv[1]\r\n",
      "\r\n",
      "if dataset not in datasets:\r\n",
      "\tsys.exit(\"wrong dataset name\")\r\n",
      "\r\n",
      "nltk.download('stopwords')\r\n",
      "stop_words = set(stopwords.words('english'))\r\n",
      "print(stop_words)\r\n",
      "\r\n",
      "# Read Word Vectors\r\n",
      "# word_vector_file = 'data/glove.6B/glove.6B.200d.txt'\r\n",
      "# vocab, embd, word_vector_map = loadWord2Vec(word_vector_file)\r\n",
      "# word_embeddings_dim = len(embd[0])\r\n",
      "# dataset = '20ng'\r\n",
      "\r\n",
      "doc_content_list = []\r\n",
      "f = open('data/corpus/' + dataset + '.txt', 'rb')\r\n",
      "# f = open('data/wiki_long_abstracts_en_text.txt', 'r')\r\n",
      "for line in f.readlines():\r\n",
      "    doc_content_list.append(line.strip().decode('latin1'))\r\n",
      "f.close()\r\n",
      "\r\n",
      "\r\n",
      "word_freq = {}  # to remove rare words\r\n",
      "\r\n",
      "for doc_content in doc_content_list:\r\n",
      "    temp = clean_str(doc_content)\r\n",
      "    words = temp.split()\r\n",
      "    for word in words:\r\n",
      "        if word in word_freq:\r\n",
      "            word_freq[word] += 1\r\n",
      "        else:\r\n",
      "            word_freq[word] = 1\r\n",
      "\r\n",
      "clean_docs = []\r\n",
      "for doc_content in doc_content_list:\r\n",
      "    temp = clean_str(doc_content)\r\n",
      "    words = temp.split()\r\n",
      "    doc_words = []\r\n",
      "    for word in words:\r\n",
      "        # word not in stop_words and word_freq[word] >= 5\r\n",
      "        if dataset == 'mr':\r\n",
      "            doc_words.append(word)\r\n",
      "        elif word not in stop_words and word_freq[word] >= 5:\r\n",
      "            doc_words.append(word)\r\n",
      "\r\n",
      "    doc_str = ' '.join(doc_words).strip()\r\n",
      "    #if doc_str == '':\r\n",
      "        #doc_str = temp\r\n",
      "    clean_docs.append(doc_str)\r\n",
      "\r\n",
      "clean_corpus_str = '\\n'.join(clean_docs)\r\n",
      "\r\n",
      "f = open('data/corpus/' + dataset + '.clean.txt', 'w')\r\n",
      "#f = open('data/wiki_long_abstracts_en_text.clean.txt', 'w')\r\n",
      "f.write(clean_corpus_str)\r\n",
      "f.close()\r\n",
      "\r\n",
      "#dataset = '20ng'\r\n",
      "min_len = 10000\r\n",
      "aver_len = 0\r\n",
      "max_len = 0 \r\n",
      "\r\n",
      "f = open('data/corpus/' + dataset + '.clean.txt', 'r')\r\n",
      "#f = open('data/wiki_long_abstracts_en_text.txt', 'r')\r\n",
      "lines = f.readlines()\r\n",
      "for line in lines:\r\n",
      "    line = line.strip()\r\n",
      "    temp = line.split()\r\n",
      "    aver_len = aver_len + len(temp)\r\n",
      "    if len(temp) < min_len:\r\n",
      "        min_len = len(temp)\r\n",
      "    if len(temp) > max_len:\r\n",
      "        max_len = len(temp)\r\n",
      "f.close()\r\n",
      "aver_len = 1.0 * aver_len / len(lines)\r\n",
      "print('min_len : ' + str(min_len))\r\n",
      "print('max_len : ' + str(max_len))\r\n",
      "print('average_len : ' + str(aver_len))\r\n"
     ]
    }
   ],
   "source": [
    "cat remove_words.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Use: python remove_words.py <dataset>",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Use: python remove_words.py <dataset>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from utils import clean_str, loadWord2Vec\n",
    "import sys\n",
    "\n",
    "if len(sys.argv) != 2:\n",
    "\tsys.exit(\"Use: python remove_words.py <dataset>\")\n",
    "\n",
    "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
    "dataset = sys.argv[1]\n",
    "\n",
    "if dataset not in datasets:\n",
    "\tsys.exit(\"wrong dataset name\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "# Read Word Vectors\n",
    "# word_vector_file = 'data/glove.6B/glove.6B.200d.txt'\n",
    "# vocab, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "# word_embeddings_dim = len(embd[0])\n",
    "# dataset = '20ng'\n",
    "\n",
    "doc_content_list = []\n",
    "f = open('data/corpus/' + dataset + '.txt', 'rb')\n",
    "# f = open('data/wiki_long_abstracts_en_text.txt', 'r')\n",
    "for line in f.readlines():\n",
    "    doc_content_list.append(line.strip().decode('latin1'))\n",
    "f.close()\n",
    "\n",
    "\n",
    "word_freq = {}  # to remove rare words\n",
    "\n",
    "for doc_content in doc_content_list:\n",
    "    temp = clean_str(doc_content)\n",
    "    words = temp.split()\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "clean_docs = []\n",
    "for doc_content in doc_content_list:\n",
    "    temp = clean_str(doc_content)\n",
    "    words = temp.split()\n",
    "    doc_words = []\n",
    "    for word in words:\n",
    "        # word not in stop_words and word_freq[word] >= 5\n",
    "        if dataset == 'mr':\n",
    "            doc_words.append(word)\n",
    "        elif word not in stop_words and word_freq[word] >= 5:\n",
    "            doc_words.append(word)\n",
    "\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "    #if doc_str == '':\n",
    "        #doc_str = temp\n",
    "    clean_docs.append(doc_str)\n",
    "\n",
    "clean_corpus_str = '\\n'.join(clean_docs)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '.clean.txt', 'w')\n",
    "#f = open('data/wiki_long_abstracts_en_text.clean.txt', 'w')\n",
    "f.write(clean_corpus_str)\n",
    "f.close()\n",
    "\n",
    "#dataset = '20ng'\n",
    "min_len = 10000\n",
    "aver_len = 0\n",
    "max_len = 0 \n",
    "\n",
    "f = open('data/corpus/' + dataset + '.clean.txt', 'r')\n",
    "#f = open('data/wiki_long_abstracts_en_text.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    temp = line.split()\n",
    "    aver_len = aver_len + len(temp)\n",
    "    if len(temp) < min_len:\n",
    "        min_len = len(temp)\n",
    "    if len(temp) > max_len:\n",
    "        max_len = len(temp)\n",
    "f.close()\n",
    "aver_len = 1.0 * aver_len / len(lines)\n",
    "print('min_len : ' + str(min_len))\n",
    "print('max_len : ' + str(max_len))\n",
    "print('average_len : ' + str(aver_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow.py           inits.py      plot_window.py   train.py\r\n",
      "build_corpus.py  layers.py     prepare_data.py  utils.py\r\n",
      "build_graph.py   metrics.py    \u001b[0m\u001b[01;34m__pycache__\u001b[0m/     visualize.py\r\n",
      "\u001b[01;34mdata\u001b[0m/            models.py     README.md        visualize_words.py\r\n",
      "doc2vec.py       plot_dim.py   remove_words.py  wordnet.py\r\n",
      "__init__.py      plot_prop.py  \u001b[01;34mresults\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\r\n",
      "import pickle as pkl\r\n",
      "import networkx as nx\r\n",
      "import scipy.sparse as sp\r\n",
      "from scipy.sparse.linalg.eigen.arpack import eigsh\r\n",
      "import sys\r\n",
      "import re\r\n",
      "\r\n",
      "\r\n",
      "def parse_index_file(filename):\r\n",
      "    \"\"\"Parse index file.\"\"\"\r\n",
      "    index = []\r\n",
      "    for line in open(filename):\r\n",
      "        index.append(int(line.strip()))\r\n",
      "    return index\r\n",
      "\r\n",
      "\r\n",
      "def sample_mask(idx, l):\r\n",
      "    \"\"\"Create mask.\"\"\"\r\n",
      "    mask = np.zeros(l)\r\n",
      "    mask[idx] = 1\r\n",
      "    return np.array(mask, dtype=np.bool)\r\n",
      "\r\n",
      "\r\n",
      "def load_data(dataset_str):\r\n",
      "    \"\"\"\r\n",
      "    Loads input data from gcn/data directory\r\n",
      "\r\n",
      "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\r\n",
      "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\r\n",
      "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\r\n",
      "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\r\n",
      "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\r\n",
      "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\r\n",
      "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\r\n",
      "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\r\n",
      "        object;\r\n",
      "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\r\n",
      "\r\n",
      "    All objects above must be saved using python pickle module.\r\n",
      "\r\n",
      "    :param dataset_str: Dataset name\r\n",
      "    :return: All data input files loaded (as well the training/test data).\r\n",
      "    \"\"\"\r\n",
      "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\r\n",
      "    objects = []\r\n",
      "    for i in range(len(names)):\r\n",
      "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\r\n",
      "            if sys.version_info > (3, 0):\r\n",
      "                objects.append(pkl.load(f, encoding='latin1'))\r\n",
      "            else:\r\n",
      "                objects.append(pkl.load(f))\r\n",
      "\r\n",
      "    x, y, tx, ty, allx, ally, graph = tuple(objects)\r\n",
      "    test_idx_reorder = parse_index_file(\r\n",
      "        \"data/ind.{}.test.index\".format(dataset_str))\r\n",
      "    test_idx_range = np.sort(test_idx_reorder)\r\n",
      "    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\r\n",
      "\r\n",
      "    # training nodes are training docs, no initial features\r\n",
      "    # print(\"x: \", x)\r\n",
      "    # test nodes are training docs, no initial features\r\n",
      "    # print(\"tx: \", tx)\r\n",
      "    # both labeled and unlabeled training instances are training docs and words\r\n",
      "    # print(\"allx: \", allx)\r\n",
      "    # training labels are training doc labels\r\n",
      "    # print(\"y: \", y)\r\n",
      "    # test labels are test doc labels\r\n",
      "    # print(\"ty: \", ty)\r\n",
      "    # ally are labels for labels for allx, some will not have labels, i.e., all 0\r\n",
      "    # print(\"ally: \\n\")\r\n",
      "    # for i in ally:\r\n",
      "    # if(sum(i) == 0):\r\n",
      "    # print(i)\r\n",
      "    # graph edge weight is the word co-occurence or doc word frequency\r\n",
      "    # no need to build map, directly build csr_matrix\r\n",
      "    # print('graph : ', graph)\r\n",
      "\r\n",
      "    if dataset_str == 'citeseer':\r\n",
      "        # Fix citeseer dataset (there are some isolated nodes in the graph)\r\n",
      "        # Find isolated nodes, add them as zero-vecs into the right position\r\n",
      "        test_idx_range_full = range(\r\n",
      "            min(test_idx_reorder), max(test_idx_reorder)+1)\r\n",
      "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\r\n",
      "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\r\n",
      "        tx = tx_extended\r\n",
      "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\r\n",
      "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\r\n",
      "        ty = ty_extended\r\n",
      "\r\n",
      "    features = sp.vstack((allx, tx)).tolil()\r\n",
      "    features[test_idx_reorder, :] = features[test_idx_range, :]\r\n",
      "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\r\n",
      "\r\n",
      "    labels = np.vstack((ally, ty))\r\n",
      "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\r\n",
      "    # print(len(labels))\r\n",
      "\r\n",
      "    idx_test = test_idx_range.tolist()\r\n",
      "    # print(idx_test)\r\n",
      "    idx_train = range(len(y))\r\n",
      "    idx_val = range(len(y), len(y)+500)\r\n",
      "\r\n",
      "    train_mask = sample_mask(idx_train, labels.shape[0])\r\n",
      "    val_mask = sample_mask(idx_val, labels.shape[0])\r\n",
      "    test_mask = sample_mask(idx_test, labels.shape[0])\r\n",
      "\r\n",
      "    y_train = np.zeros(labels.shape)\r\n",
      "    y_val = np.zeros(labels.shape)\r\n",
      "    y_test = np.zeros(labels.shape)\r\n",
      "    y_train[train_mask, :] = labels[train_mask, :]\r\n",
      "    y_val[val_mask, :] = labels[val_mask, :]\r\n",
      "    y_test[test_mask, :] = labels[test_mask, :]\r\n",
      "\r\n",
      "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\r\n",
      "\r\n",
      "\r\n",
      "def load_corpus(dataset_str):\r\n",
      "    \"\"\"\r\n",
      "    Loads input corpus from gcn/data directory\r\n",
      "\r\n",
      "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\r\n",
      "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\r\n",
      "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\r\n",
      "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\r\n",
      "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\r\n",
      "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\r\n",
      "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\r\n",
      "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\r\n",
      "    ind.dataset_str.train.index => the indices of training docs in original doc list.\r\n",
      "\r\n",
      "    All objects above must be saved using python pickle module.\r\n",
      "\r\n",
      "    :param dataset_str: Dataset name\r\n",
      "    :return: All data input files loaded (as well the training/test data).\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\r\n",
      "    objects = []\r\n",
      "    for i in range(len(names)):\r\n",
      "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\r\n",
      "            if sys.version_info > (3, 0):\r\n",
      "                objects.append(pkl.load(f, encoding='latin1'))\r\n",
      "            else:\r\n",
      "                objects.append(pkl.load(f))\r\n",
      "\r\n",
      "    x, y, tx, ty, allx, ally, adj = tuple(objects)\r\n",
      "    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\r\n",
      "\r\n",
      "    features = sp.vstack((allx, tx)).tolil()\r\n",
      "    labels = np.vstack((ally, ty))\r\n",
      "    print(len(labels))\r\n",
      "\r\n",
      "    train_idx_orig = parse_index_file(\r\n",
      "        \"data/{}.train.index\".format(dataset_str))\r\n",
      "    train_size = len(train_idx_orig)\r\n",
      "\r\n",
      "    val_size = train_size - x.shape[0]\r\n",
      "    test_size = tx.shape[0]\r\n",
      "\r\n",
      "    idx_train = range(len(y))\r\n",
      "    idx_val = range(len(y), len(y) + val_size)\r\n",
      "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\r\n",
      "\r\n",
      "    train_mask = sample_mask(idx_train, labels.shape[0])\r\n",
      "    val_mask = sample_mask(idx_val, labels.shape[0])\r\n",
      "    test_mask = sample_mask(idx_test, labels.shape[0])\r\n",
      "\r\n",
      "    y_train = np.zeros(labels.shape)\r\n",
      "    y_val = np.zeros(labels.shape)\r\n",
      "    y_test = np.zeros(labels.shape)\r\n",
      "    y_train[train_mask, :] = labels[train_mask, :]\r\n",
      "    y_val[val_mask, :] = labels[val_mask, :]\r\n",
      "    y_test[test_mask, :] = labels[test_mask, :]\r\n",
      "\r\n",
      "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\r\n",
      "\r\n",
      "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\r\n",
      "\r\n",
      "\r\n",
      "def sparse_to_tuple(sparse_mx):\r\n",
      "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\r\n",
      "    def to_tuple(mx):\r\n",
      "        if not sp.isspmatrix_coo(mx):\r\n",
      "            mx = mx.tocoo()\r\n",
      "        coords = np.vstack((mx.row, mx.col)).transpose()\r\n",
      "        values = mx.data\r\n",
      "        shape = mx.shape\r\n",
      "        return coords, values, shape\r\n",
      "\r\n",
      "    if isinstance(sparse_mx, list):\r\n",
      "        for i in range(len(sparse_mx)):\r\n",
      "            sparse_mx[i] = to_tuple(sparse_mx[i])\r\n",
      "    else:\r\n",
      "        sparse_mx = to_tuple(sparse_mx)\r\n",
      "\r\n",
      "    return sparse_mx\r\n",
      "\r\n",
      "\r\n",
      "def preprocess_features(features):\r\n",
      "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\r\n",
      "    rowsum = np.array(features.sum(1))\r\n",
      "    r_inv = np.power(rowsum, -1).flatten()\r\n",
      "    r_inv[np.isinf(r_inv)] = 0.\r\n",
      "    r_mat_inv = sp.diags(r_inv)\r\n",
      "    features = r_mat_inv.dot(features)\r\n",
      "    return sparse_to_tuple(features)\r\n",
      "\r\n",
      "\r\n",
      "def normalize_adj(adj):\r\n",
      "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\r\n",
      "    adj = sp.coo_matrix(adj)\r\n",
      "    rowsum = np.array(adj.sum(1))\r\n",
      "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\r\n",
      "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\r\n",
      "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\r\n",
      "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\r\n",
      "\r\n",
      "\r\n",
      "def preprocess_adj(adj):\r\n",
      "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\r\n",
      "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\r\n",
      "    return sparse_to_tuple(adj_normalized)\r\n",
      "\r\n",
      "\r\n",
      "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\r\n",
      "    \"\"\"Construct feed dictionary.\"\"\"\r\n",
      "    feed_dict = dict()\r\n",
      "    feed_dict.update({placeholders['labels']: labels})\r\n",
      "    feed_dict.update({placeholders['labels_mask']: labels_mask})\r\n",
      "    feed_dict.update({placeholders['features']: features})\r\n",
      "    feed_dict.update({placeholders['support'][i]: support[i]\r\n",
      "                      for i in range(len(support))})\r\n",
      "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\r\n",
      "    return feed_dict\r\n",
      "\r\n",
      "\r\n",
      "def chebyshev_polynomials(adj, k):\r\n",
      "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\r\n",
      "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\r\n",
      "\r\n",
      "    adj_normalized = normalize_adj(adj)\r\n",
      "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\r\n",
      "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\r\n",
      "    scaled_laplacian = (\r\n",
      "        2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\r\n",
      "\r\n",
      "    t_k = list()\r\n",
      "    t_k.append(sp.eye(adj.shape[0]))\r\n",
      "    t_k.append(scaled_laplacian)\r\n",
      "\r\n",
      "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\r\n",
      "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\r\n",
      "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\r\n",
      "\r\n",
      "    for i in range(2, k+1):\r\n",
      "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\r\n",
      "\r\n",
      "    return sparse_to_tuple(t_k)\r\n",
      "\r\n",
      "\r\n",
      "def loadWord2Vec(filename):\r\n",
      "    \"\"\"Read Word Vectors\"\"\"\r\n",
      "    vocab = []\r\n",
      "    embd = []\r\n",
      "    word_vector_map = {}\r\n",
      "    file = open(filename, 'r')\r\n",
      "    for line in file.readlines():\r\n",
      "        row = line.strip().split(' ')\r\n",
      "        if(len(row) > 2):\r\n",
      "            vocab.append(row[0])\r\n",
      "            vector = row[1:]\r\n",
      "            length = len(vector)\r\n",
      "            for i in range(length):\r\n",
      "                vector[i] = float(vector[i])\r\n",
      "            embd.append(vector)\r\n",
      "            word_vector_map[row[0]] = vector\r\n",
      "    print('Loaded Word Vectors!')\r\n",
      "    file.close()\r\n",
      "    return vocab, embd, word_vector_map\r\n",
      "\r\n",
      "def clean_str(string):\r\n",
      "    \"\"\"\r\n",
      "    Tokenization/string cleaning for all datasets except for SST.\r\n",
      "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\r\n",
      "    \"\"\"\r\n",
      "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\r\n",
      "    string = re.sub(r\"\\'s\", \" \\'s\", string)\r\n",
      "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\r\n",
      "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\r\n",
      "    string = re.sub(r\"\\'re\", \" \\'re\", string)\r\n",
      "    string = re.sub(r\"\\'d\", \" \\'d\", string)\r\n",
      "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\r\n",
      "    string = re.sub(r\",\", \" , \", string)\r\n",
      "    string = re.sub(r\"!\", \" ! \", string)\r\n",
      "    string = re.sub(r\"\\(\", \" \\( \", string)\r\n",
      "    string = re.sub(r\"\\)\", \" \\) \", string)\r\n",
      "    string = re.sub(r\"\\?\", \" \\? \", string)\r\n",
      "    string = re.sub(r\"\\s{2,}\", \" \", string)\r\n",
      "    return string.strip().lower()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\n",
    "        \"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "    # training nodes are training docs, no initial features\n",
    "    # print(\"x: \", x)\n",
    "    # test nodes are training docs, no initial features\n",
    "    # print(\"tx: \", tx)\n",
    "    # both labeled and unlabeled training instances are training docs and words\n",
    "    # print(\"allx: \", allx)\n",
    "    # training labels are training doc labels\n",
    "    # print(\"y: \", y)\n",
    "    # test labels are test doc labels\n",
    "    # print(\"ty: \", ty)\n",
    "    # ally are labels for labels for allx, some will not have labels, i.e., all 0\n",
    "    # print(\"ally: \\n\")\n",
    "    # for i in ally:\n",
    "    # if(sum(i) == 0):\n",
    "    # print(i)\n",
    "    # graph edge weight is the word co-occurence or doc word frequency\n",
    "    # no need to build map, directly build csr_matrix\n",
    "    # print('graph : ', graph)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(\n",
    "            min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "    # print(len(labels))\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    # print(idx_test)\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def load_corpus(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input corpus from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    labels = np.vstack((ally, ty))\n",
    "    print(len(labels))\n",
    "\n",
    "    train_idx_orig = parse_index_file(\n",
    "        \"data/{}.train.index\".format(dataset_str))\n",
    "    train_size = len(train_idx_orig)\n",
    "\n",
    "    val_size = train_size - x.shape[0]\n",
    "    test_size = tx.shape[0]\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + val_size)\n",
    "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return sparse_to_tuple(features)\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i]\n",
    "                      for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (\n",
    "        2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return sparse_to_tuple(t_k)\n",
    "\n",
    "\n",
    "def loadWord2Vec(filename):\n",
    "    \"\"\"Read Word Vectors\"\"\"\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    word_vector_map = {}\n",
    "    file = open(filename, 'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        if(len(row) > 2):\n",
    "            vocab.append(row[0])\n",
    "            vector = row[1:]\n",
    "            length = len(vector)\n",
    "            for i in range(length):\n",
    "                vector[i] = float(vector[i])\n",
    "            embd.append(vector)\n",
    "            word_vector_map[row[0]] = vector\n",
    "    print('Loaded Word Vectors!')\n",
    "    file.close()\n",
    "    return vocab, embd, word_vector_map\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "â€‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import division\r\n",
      "from __future__ import print_function\r\n",
      "\r\n",
      "import time\r\n",
      "import tensorflow as tf\r\n",
      "\r\n",
      "from sklearn import metrics\r\n",
      "from utils import *\r\n",
      "from models import GCN, MLP\r\n",
      "import random\r\n",
      "import os\r\n",
      "import sys\r\n",
      "\r\n",
      "if len(sys.argv) != 2:\r\n",
      "\tsys.exit(\"Use: python train.py <dataset>\")\r\n",
      "\r\n",
      "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\r\n",
      "dataset = sys.argv[1]\r\n",
      "\r\n",
      "if dataset not in datasets:\r\n",
      "\tsys.exit(\"wrong dataset name\")\r\n",
      "\r\n",
      "\r\n",
      "# Set random seed\r\n",
      "seed = random.randint(1, 200)\r\n",
      "np.random.seed(seed)\r\n",
      "tf.set_random_seed(seed)\r\n",
      "\r\n",
      "# Settings\r\n",
      "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n",
      "\r\n",
      "flags = tf.app.flags\r\n",
      "FLAGS = flags.FLAGS\r\n",
      "# 'cora', 'citeseer', 'pubmed'\r\n",
      "flags.DEFINE_string('dataset', dataset, 'Dataset string.')\r\n",
      "# 'gcn', 'gcn_cheby', 'dense'\r\n",
      "flags.DEFINE_string('model', 'gcn', 'Model string.')\r\n",
      "flags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\r\n",
      "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\r\n",
      "flags.DEFINE_integer('hidden1', 200, 'Number of units in hidden layer 1.')\r\n",
      "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\r\n",
      "flags.DEFINE_float('weight_decay', 0,\r\n",
      "                   'Weight for L2 loss on embedding matrix.')  # 5e-4\r\n",
      "flags.DEFINE_integer('early_stopping', 10,\r\n",
      "                     'Tolerance for early stopping (# of epochs).')\r\n",
      "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\r\n",
      "\r\n",
      "# Load data\r\n",
      "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(\r\n",
      "    FLAGS.dataset)\r\n",
      "print(adj)\r\n",
      "# print(adj[0], adj[1])\r\n",
      "features = sp.identity(features.shape[0])  # featureless\r\n",
      "\r\n",
      "print(adj.shape)\r\n",
      "print(features.shape)\r\n",
      "\r\n",
      "# Some preprocessing\r\n",
      "features = preprocess_features(features)\r\n",
      "if FLAGS.model == 'gcn':\r\n",
      "    support = [preprocess_adj(adj)]\r\n",
      "    num_supports = 1\r\n",
      "    model_func = GCN\r\n",
      "elif FLAGS.model == 'gcn_cheby':\r\n",
      "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\r\n",
      "    num_supports = 1 + FLAGS.max_degree\r\n",
      "    model_func = GCN\r\n",
      "elif FLAGS.model == 'dense':\r\n",
      "    support = [preprocess_adj(adj)]  # Not used\r\n",
      "    num_supports = 1\r\n",
      "    model_func = MLP\r\n",
      "else:\r\n",
      "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\r\n",
      "\r\n",
      "# Define placeholders\r\n",
      "placeholders = {\r\n",
      "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\r\n",
      "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\r\n",
      "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\r\n",
      "    'labels_mask': tf.placeholder(tf.int32),\r\n",
      "    'dropout': tf.placeholder_with_default(0., shape=()),\r\n",
      "    # helper variable for sparse dropout\r\n",
      "    'num_features_nonzero': tf.placeholder(tf.int32)\r\n",
      "}\r\n",
      "\r\n",
      "# Create model\r\n",
      "print(features[2][1])\r\n",
      "model = model_func(placeholders, input_dim=features[2][1], logging=True)\r\n",
      "\r\n",
      "# Initialize session\r\n",
      "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\r\n",
      "sess = tf.Session(config=session_conf)\r\n",
      "\r\n",
      "\r\n",
      "# Define model evaluation function\r\n",
      "def evaluate(features, support, labels, mask, placeholders):\r\n",
      "    t_test = time.time()\r\n",
      "    feed_dict_val = construct_feed_dict(\r\n",
      "        features, support, labels, mask, placeholders)\r\n",
      "    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\r\n",
      "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\r\n",
      "\r\n",
      "\r\n",
      "# Init variables\r\n",
      "sess.run(tf.global_variables_initializer())\r\n",
      "\r\n",
      "cost_val = []\r\n",
      "\r\n",
      "# Train model\r\n",
      "for epoch in range(FLAGS.epochs):\r\n",
      "\r\n",
      "    t = time.time()\r\n",
      "    # Construct feed dictionary\r\n",
      "    feed_dict = construct_feed_dict(\r\n",
      "        features, support, y_train, train_mask, placeholders)\r\n",
      "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\r\n",
      "\r\n",
      "    # Training step\r\n",
      "    outs = sess.run([model.opt_op, model.loss, model.accuracy,\r\n",
      "                     model.layers[0].embedding], feed_dict=feed_dict)\r\n",
      "\r\n",
      "    # Validation\r\n",
      "    cost, acc, pred, labels, duration = evaluate(\r\n",
      "        features, support, y_val, val_mask, placeholders)\r\n",
      "    cost_val.append(cost)\r\n",
      "\r\n",
      "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\r\n",
      "          \"train_acc=\", \"{:.5f}\".format(\r\n",
      "              outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\r\n",
      "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\r\n",
      "\r\n",
      "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\r\n",
      "        print(\"Early stopping...\")\r\n",
      "        break\r\n",
      "\r\n",
      "print(\"Optimization Finished!\")\r\n",
      "\r\n",
      "# Testing\r\n",
      "test_cost, test_acc, pred, labels, test_duration = evaluate(\r\n",
      "    features, support, y_test, test_mask, placeholders)\r\n",
      "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\r\n",
      "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\r\n",
      "\r\n",
      "test_pred = []\r\n",
      "test_labels = []\r\n",
      "print(len(test_mask))\r\n",
      "for i in range(len(test_mask)):\r\n",
      "    if test_mask[i]:\r\n",
      "        test_pred.append(pred[i])\r\n",
      "        test_labels.append(labels[i])\r\n",
      "\r\n",
      "print(\"Test Precision, Recall and F1-Score...\")\r\n",
      "print(metrics.classification_report(test_labels, test_pred, digits=4))\r\n",
      "print(\"Macro average Test Precision, Recall and F1-Score...\")\r\n",
      "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\r\n",
      "print(\"Micro average Test Precision, Recall and F1-Score...\")\r\n",
      "print(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\r\n",
      "\r\n",
      "# doc and word embeddings\r\n",
      "print('embeddings:')\r\n",
      "word_embeddings = outs[3][train_size: adj.shape[0] - test_size]\r\n",
      "train_doc_embeddings = outs[3][:train_size]  # include val docs\r\n",
      "test_doc_embeddings = outs[3][adj.shape[0] - test_size:]\r\n",
      "\r\n",
      "print(len(word_embeddings), len(train_doc_embeddings),\r\n",
      "      len(test_doc_embeddings))\r\n",
      "print(word_embeddings)\r\n",
      "\r\n",
      "f = open('data/corpus/' + dataset + '_vocab.txt', 'r')\r\n",
      "words = f.readlines()\r\n",
      "f.close()\r\n",
      "\r\n",
      "vocab_size = len(words)\r\n",
      "word_vectors = []\r\n",
      "for i in range(vocab_size):\r\n",
      "    word = words[i].strip()\r\n",
      "    word_vector = word_embeddings[i]\r\n",
      "    word_vector_str = ' '.join([str(x) for x in word_vector])\r\n",
      "    word_vectors.append(word + ' ' + word_vector_str)\r\n",
      "\r\n",
      "word_embeddings_str = '\\n'.join(word_vectors)\r\n",
      "f = open('data/' + dataset + '_word_vectors.txt', 'w')\r\n",
      "f.write(word_embeddings_str)\r\n",
      "f.close()\r\n",
      "\r\n",
      "doc_vectors = []\r\n",
      "doc_id = 0\r\n",
      "for i in range(train_size):\r\n",
      "    doc_vector = train_doc_embeddings[i]\r\n",
      "    doc_vector_str = ' '.join([str(x) for x in doc_vector])\r\n",
      "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\r\n",
      "    doc_id += 1\r\n",
      "\r\n",
      "for i in range(test_size):\r\n",
      "    doc_vector = test_doc_embeddings[i]\r\n",
      "    doc_vector_str = ' '.join([str(x) for x in doc_vector])\r\n",
      "    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\r\n",
      "    doc_id += 1\r\n",
      "\r\n",
      "doc_embeddings_str = '\\n'.join(doc_vectors)\r\n",
      "f = open('data/' + dataset + '_doc_vectors.txt', 'w')\r\n",
      "f.write(doc_embeddings_str)\r\n",
      "f.close()\r\n"
     ]
    }
   ],
   "source": [
    "cat train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
